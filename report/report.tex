\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{setspace}
\doublespacing
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}


\begin{document}
%
\title{GMRES-based Iterative Refinement for
Solving Linear Systems}


\author{Shutong Wu\thanks{wu867@mcmaster.ca}, Valerie Vreugdenhil\thanks{vreugdev@mcmaster.ca}, Gaofeng Zhou\thanks{zhoug28@mcmaster.ca}}
%
%
\maketitle             

%
\section{Introduction}

\section{Literature Review}

\subsection{Key Concepts}
\subsubsection{Iterative Refinement}
\subsubsection{GMRES-IR}
\subsubsection{Mixed Precision}

\subsection{Comparative Analysis of GMRES-IR and LU-IR}
This section conducts a comparative analysis of GMRES-based IR(GMRES-IR) and LU-based IR(LU-IR) in terms of accuracy, convergence, and performance in a mixed-precision setting.
\subsubsection{LU-IR}
Iterative refinement has traditionally used $LU$ factorization as the inner solver. A basic iterative refinement algorithm using $LU$ factorization can be described as follows:

\begin{algorithm}
    \caption{LU-IR. $A \in \mathbb{R}^{n \times n}$ is nonsingular and $b \in \mathbb{R}^n$. Three precisions are used, satisfying $u_r \leq u \leq u_l$.}
    \begin{algorithmic}[1]
        \State Compute the factorization $A = LU$ in precision $u_l$.
        \State Solve $LUx_1 = b$ by substitution in precision $u_l$.
        \For{$i = 1:i_{\max}$ or until converged}
            \State Compute $r_i = b - Ax_i$ in precision $u_r$.
            \State Solve $LUd_i = r_i$ by substitution in precision $u_l$.
            \State Update $x_{i+1} = x_i + d_i$ in precision $u$.
        \EndFor
    \end{algorithmic}
\end{algorithm}

Here the most computational heavy step is the $LU$ factorization, which costs $O(n^3)$ flop. The substitutions on line 5 costs in total $O(Kn^2)$ flops, which is negligible compared to $LU$ factorization for large $n$ and reasonable $K$ ($K$ is the total number of iterations). So by adopting a lower precision for $u_l$, the computational cost can be greatly reduced. With half-precision availability ($u_{16}$), the potential speedup with $u=u_r=u_{64}$ and $u_l=u_{16}$ on an NVIDIA P100 GPU can reach 2.7. 

The convergence of LU-IR depends on various factors, including numerical stability of the $LU$ decomposition, precision scheme, and also the condition of $A$. Specifically, with $\kappa(A)u_l \ll 1$ and a numerically stable $LU$ decomposition, convergence is guaranteed. However, when lower precision is applied (especially in half-precision schemes), the requirement for $\kappa(A)$ tightens. For instance, with fp16, the unit round-off $u_{16}=u_l\approx 2^{-11}$, rendering $\kappa(A)\ll 2000$. With bfloat16, the unit round-off $u_{16}=u_l\approx 2^{-8}$, giving an even tighter bound $\kappa(A)\ll 300$.

The above algorithm achieves a relative error bounded approximately by $3n\||A^{-1}||\hat{L}||\hat{U}|\|u_l$.


\subsubsection{GMRES-IR}
The GMRES-based iterative refinement (GMRES-IR) in a mixed-precision setting can be described as follows:

\begin{algorithm}
    \caption{GMRES-IR. $A \in \mathbb{R}^{n \times n}$ is nonsingular and $b \in \mathbb{R}^n$. Five precisions are used: $u_r$, $u_g$, $u_p$, $u$, $u_l$.}
    \begin{algorithmic}[1]
        \State Compute the factorization $A = LU$ in precision $u_l$.
        \State Solve $LUx_1 = b$ by substitution in precision $u_l$.
        \For{$i = 1:i_{\max}$ or until converged}
            \State Compute $r_i = b - Ax_i$ in precision $u_r$.
            \State Solve $U^{-1}L^{-1}Ad_i = U^{-1}L^{-1}r_i$ by GMRES in precision $u_g$, performing the products with $U^{-1}L^{-1}A$ in precision $u_p$.
            \State Compute $x_{i+1} = x_i + d_i$ in precision $u$.
        \EndFor
    \end{algorithmic}
\end{algorithm}

An important feature of GMRES-IR is its applicability to extremely ill-condition matrices.

In algorithm 2 line 4 , denote the relative error:
\[
    \xi_i = \frac{\|d_i - \hat{d_i}\|_{\infty}}{\|d_i\|_{\infty}}
\]
Let
\[
\mu_i = \frac{\|A(\mathbf{x}_i - \hat{\mathbf{x}}_i)\|_{\infty}}{\|A\|_{\infty}\|\mathbf{x}_1 - \hat{\mathbf{x}}_i\|_{\infty}} \leq 1,
\]
\[
\phi_i = 2 \min(\text{cond}(A), \kappa_{\infty}(A)\mu_i) u_l + \xi_i,
\]
where cond(A) = $\||A^{-1}||A||x|\|_\infty$
$\xi_i$ is reduced at each iteration until an iterate $\hat{x}$ is obtained such that:
\[
    \frac{\|x - \hat{x}\|_{\infty}}{\|x\|_{\infty}} \lesssim u+4p\ cond(A,x)u_r
\]
where $p$ is the maximum number of nonzeros in any row of $[A\ b]$. This accuracy bound depends on precisions $u$ and $u_r$, and is independent of precision $u_l$ and conditioning of the problem. This motivates adopting a higher precision for the residual $u_r$. 





\section{Experimental Setup}
\section{Result and Analysis}














\subsection{Key Features of GMRES}
\subsection{Introduction}
The Generalized Minimal Residual Method (GMRES) is one of the significant methods for solving linear algebraic systems with non-symmetric matrices .  \\
\begin{equation}
    Ax=b
\end{equation}
Assume that $\textbf{A}$ is a real nonsingular $\textbf{N}$ by$\textbf{N}$ matrix and $b$ is a real vector. Given an initial guess $x^0$ for the solution, the algorithm generates approximate solutions $x^n,n=1,2,...,\textbf{N}$ from the linear variety
\begin{equation}
    x^0+\textbf{K}_n(\textbf{A},r^o)
\end{equation}
minimizing the Euclidean norm of the residual,
\begin{equation}
    \| \mathbf{b} - A\mathbf{x}^n \| = \min_{\mathbf{u} \in \mathbf{x}_0 + \mathcal{K}_n(A,r_0)} \| \mathbf{b} - A\mathbf{u} \|,
\end{equation}

where $r^0=b-\textbf{A}x^0$ is the initial residual and $\textsc{K}_n(\textbf{A},r^0)$ is the $n-th$ Krylov subspace generated by \textbf{A},$r^0$,
\begin{equation}
    \mathcal{K}_n(\mathbf{A},r^0)=span{r^0,\mathbf{A}r^0,...,\mathbf{A}^(n-1)r^0}.
\end{equation}


\subsection{Key Features}

\bibliographystyle{refs-style}
\bibliography{refs}
%
\end{document}