
\section{Introduction}
In the realm of scientific computing, solving linear systems $Ax=b$ with high efficiency and performance is of paramount importance, particularly in the context of large-scale computing applications. Iterative Refinement (IR), as a long-studied solver to linear equations, enhances the precision of solutions obtained by initial approximations in an iterative manner, and has proven essential in a mixed precision setting. Generalized Minimal Residual Method (GMRES) and LU Decomposition (LU) represent two foundational approaches adapted with IR to form GMRES-IR and LU-IR, respectively. LU-IR applies IR to the results of LU factorizations, offering a direct solution pathway that is often faster but may struggle with error propagation in lower precision environments. GMRES-IR combines the robustness of GMRES with the flexible scheme of mixed-precision IR, making it suitable for solving ill-conditioned systems with stability and efficiency. This report delves into these methods, particularly comparing GMRES-IR and LU-IR, to discern their effectiveness in terms of accuracy, convergence, and computational performance within mixed precision frameworks.

\subsection{Iterative Refinement}
Iterative Refinement (IR) is a technique used to enhance the accuracy of a solution to a system of linear equations in an iterative manner. The core idea behind IR is to iteratively correct residual errors $r$ in a preliminary solution through less computationally intensive methods, thereby improving both accuracy and performance. IR has proven itself particularly useful in mixed precision contexts.\\

Consider a linear system represented by:
\begin{equation}
Ax=b
\end{equation}
where A is a non-singular matrix, x is the vector of unknowns, and b is the known right-hand side vector. The iterative refinement process begins with an initial approximate solution $x_0$, which may be obtained by any direct or indirect solver (e.g. with LU factorization). The refinement process then proceeds by identifying and correcting the residual error in the solution repeatedly until certain tolerance condition is met.
The residual error $r_i$ in round $i$ is computed by:
\begin{equation}
r_i = b - Ax_{i-1}
\end{equation}
To improve the solution, a correction $d_i$ is obtained by solving:
\begin{equation}
Ad_i = r_i
\end{equation}
Adding the correction term $d_i$ to the accumulative solution $x_{i-1}$, we refine the solution:
\begin{equation}
x_i =x_{i-1} + d_i
\end{equation}

The whole process can be summarized as the below algorithm.
\begin{algorithm}
\caption{IR. $A \in \mathbb{R}^{n \times n}$ is nonsingular, $b \in \mathbb{R}^n$}
\begin{algorithmic}[1]
    \State Compute initial approximation $x_0$
    \For{$i = 1, 2, \ldots, i_{\max}$ or until convergence}
        \State Compute the residual $r_i = b - Ax_{i-1}$
        \State Solve the correction equation $Ad_i = r_i$ using a suitable solver
        \State Update the solution $x_i = x_{i-1} + d_i$
    \EndFor
    \State \textbf{return} $x_i$ as the refined solution
\end{algorithmic}
\end{algorithm}

Traditionally, LU factorization is adopted as the solver for $x_0$ (line 1) and corrections $d_i$ (line 4). This gives LU-IR. Detailed analysis of LU-IR is given in \ref{subsec:comparison}.


\subsection{GMRES}
Generalized Minimal Residual Method (GMRES) and its variants are advanced numerical techniques for solving non-symmetric linear systems based on iterative refinement (IR). This method is particularly useful for large or ill-conditioned matrices where direct methods are inefficient or impractical. \\
As an iterative method for solving non-symmetric linear systems, GMRES seeks to minimize the residual over a Krylov subspace generated by the matrix and the residual vector. The main steps in a GMRES iteration include\cite{Homer1988}:
\begin{itemize}
    \item Using the Arnoldi process to construct an orthogonal basis for the Krylov subspace
    \item Updating the QR factorization of the resulting Hessenberg matrix
    \item Solving the resulting least squares problem to update the solution
\end{itemize}

Consider a linear system represented by:
\begin{equation}
    Ax=b
\end{equation}
Assume that $\textbf{A}$ is a real nonsingular $\textbf{N}$ by $\textbf{N}$ matrix and $b$ is a real vector. Given an initial guess $x_0$ for the solution, the algorithm generates approximate solutions $x_n,n=1,2,...,\textbf{N}$ from the linear variety
\begin{equation}
    x_0+\textbf{K}_n(\textbf{A},r_0)
\end{equation}
minimizing the Euclidean norm of the residual
\begin{equation}
    \| \mathbf{b} - A\mathbf{x}_n \| = \min_{\mathbf{u} \in \mathbf{x}_0 + \mathcal{K}_n(A,r_0)} \| \mathbf{b} - A\mathbf{u} \|,
\end{equation}

where $r_0=b-\textbf{A}x_0$ is the initial residual and $\textsc{K}_n(\textbf{A},r_0)$ is the $n-th$ Krylov subspace generated by \textbf{A}, $r_0$
\begin{equation}
    K_n(A, r_0) = \text{span}\{r_0, Ar_0, \dots, A^{n-1}r_0\}.
\end{equation}
Clearly, $r_n \in r_0 + A K_n(A, r_0)$. We call $\textbf{AK}_{n}(\textbf{A},r_0)$ a Krylov residual subspace. \\
Such an approximation always exist and is unique, though it can be computed in many different ways. \\
Most methods for computing the approximation $x_n$ satisfying (7) start by constructing an orthonormal basis, called an Arnoldi basis, for the Krylov subspace  (8). The recurrence for the basis vectors can be written in matrix for as 
\begin{equation}
    AV_n=V_{n+1}H_{n+1,n}
\end{equation}
where $V_{n+1}$ is the $N$ by $(n+1)$ matrix with the orthonormal basis vectors $V_1,V_2,\dots,V_{n+1}$ as its columns and $H_{n+1,n}$ is the $(n+1)$ by $n$ upper Hesenberg matrix of the orthogonalization (and normalization) coefficients, $n<N$. 
The approximate solution $x_n$ is then taken to be of the form $x_n=x_0+V_ny_n$, where $y_n$ is chosen to minimize
\begin{equation}
    \| \mathbf{b} - A\mathbf{x}_n \| = \| \mathbf{r}_0 - AV_n\mathbf{y}_n \| \\
= \| V_{n+1}(\mathbf{e}_1 - H_{n+1,n}\mathbf{y}_n) \| \\
= \| (\mathbf{e}_1 - H_{n+1,n}\mathbf{y}_n) \|.
\end{equation}
The problem of solving approximately the original $N$-dimensional system $Ax=b$ is thus transformed to the $n$-dimensional least squares problem
\begin{equation}
    \| \mathbf{e}_1 - H_{n+1,n}\mathbf{y}_n \| = \min_{\mathbf{y}} \| \mathbf{e}_1 - H_{n+1,n}\mathbf{y} \|, \quad \mathbf{x}_n = \mathbf{x}_0 + V_n\mathbf{y}_n.
\end{equation}
We call $b-Ax_n$ a true residual and $\rho e_1 - H_{n+1,n}y_n$ an Arnoldi residual, where $\rho = \|\mathbf{r}_0\|$.\\

In Algorithm 1, if GMRES is used as solver for $x_0$ and corrections $d_i$, the algorithm is called GMRES-based IR (GMRES-IR). Detailed analysis of GMRES-IR is given in \ref{subsec:comparison}.

\subsection{Mixed Precision Computing}
Mixed precision computing has emerged as a highly effective strategy in numerical computing, 
particularly for enhancing the performance and efficiency of iterative refinement (IR) techniques in solving systems of linear equations. 
This approach leverages computational hardware optimally by using different precision levels for different parts of the computation, 
thus balancing between performance and overall accuracy.
The core idea behind mixed precision computing is to utilize lower precision for less critical computations and higher precision for more sensitive operations, 
thus `saving` precision where it is not needed and `spending` it where it is essential.

With the advent of modern processors capable of efficiently handling varying precisionsâ€”from single (FP32) and double (FP64) to even lower precision formats like half-precision (FP16), 
mixed precision strategies have become increasingly relevant. 

In mixed precision IR, the initial solution and correction computation might be performed in lower precision to exploit faster arithmetic operations. 
While for critical steps, particularly the residual calculation, it may utilize higher precision to ensure that the refinement process robustly improves the solution without being affected by numerical instability.

To denote the precision levels used in mixed precision computing, 
we use unit round-off $u$, also called machine epsilon, as a label for the different precisions. 
For example, $u_{16}$ denote the use of half-precision (FP16)($\approx 2^{-11}$).
\clearpage